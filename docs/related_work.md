<!--
 * @Author: hibana2077 hibana2077@gmail.com
 * @Date: 2024-06-21 10:37:03
 * @LastEditors: hibana2077 hibana2077@gmail.com
 * @LastEditTime: 2024-06-22 11:29:33
 * @FilePath: \Dataset-Cartography-for-Tree-based-model\docs\related_work.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
### 2. Related Work

#### 2.1. Overview of Data Cartography

In the realm of machine learning, particularly with regard to the development and optimization of tree-based models such as decision trees and random forests, recent studies have expanded our understanding of data cartography and its applications. Data Cartography initially emerged as a method for mapping data points based on their difficulty levels during neural network training, specifically for text classification. Its adaptation to tree-based models marks a significant innovation, enhancing the performance of these models by focusing training efforts on the most informative instances.

One notable advancement in this field is the Cartography Active Learning (CAL), which employs model behavior during training to identify critical data points for labeling, as demonstrated by Zhang and Plank in 2021 ([Zhang & Plank, 2021](https://consensus.app/papers/cartography-active-learning-zhang/d7e62d8c35215cbda66edd8042ea4e82/?utm_source=chatgpt)). This strategy competes favorably with traditional active learning methods by using data maps to gain insights into dataset quality, thus improving the efficiency of model training.

Moreover, the versatility and robustness of CART models in predictive modeling have been highlighted in various domains. For example, Rani et al. (2023) found that CART models outperform linear models in predicting innovation efforts within firms, showcasing the broad applicability of these models across different sectors ([Rani et al., 2023](https://consensus.app/papers/machine-learning-model-predicting-effort-firms-rani/7762df2cbd4a5720a9446e960657021b/?utm_source=chatgpt)).

Further enhancements have been made through the integration of CART with other modeling approaches, such as fuzzy ARTMAP neural networks. This hybrid modeling approach, explored by Seera et al. in 2015, stabilizes data learning while providing interpretable decision rules, particularly in medical data classification, underscoring the potential of combining CART with other learning paradigms ([Seera et al., 2015](https://consensus.app/papers/fam–cart-model-application-data-classification-seera/135e75eea34c535e858f99888489815b/?utm_source=chatgpt)).

Additionally, the efficiency of tree-based models has been improved through runtime optimizations. Techniques such as cache-conscious data structuring and micro-batching predictions, discussed by Asadi et al. in 2014, have significantly boosted the performance of models like gradient-boosted regression trees ([Asadi et al., 2014](https://consensus.app/papers/optimizations-treebased-machine-learning-models-asadi/5bf2a386dbcb5790bcb4aeac49a66950/?utm_source=chatgpt)).

Beyond these technical advancements, the concept of Dataset Cartography, as proposed by Swabha in 2020, utilizes "Data Maps" to diagnose and characterize large datasets through model-based assessments. This approach highlights three distinct regions within datasets—ambiguous, easy-to-learn, and hard-to-learn—that each play a crucial role in model training and performance. By focusing on model confidence and its variability over training epochs, this methodology suggests a paradigm shift from prioritizing data quantity to enhancing data quality, aiming for robust models with improved generalization capabilities outside of the training distribution ([Swabha, 2020](https://aclanthology.org/2020.emnlp-main.746)).

Together, these studies underscore the dynamic evolution of machine learning methodologies and their increasing sophistication in addressing complex and varied data challenges.

#### 2.2. Tree-Based Models in Machine Learning

In the realm of tree-based machine learning models, significant strides have been made across various dimensions such as optimization, interpretability, and application. [(Asadi et al., 2014)](https://consensus.app/papers/optimizations-treebased-machine-learning-models-asadi/5bf2a386dbcb5790bcb4aeac49a66950/?utm_source=chatgpt) delve into runtime optimizations specifically for gradient-boosted regression trees, aiming to leverage modern processor architectures to enhance performance. In a similar vein, [(Yan et al., 2022)](https://consensus.app/papers/distributed-training-tree-models-yan/00d26ba980365b9382c9631e8666e7e6/?utm_source=chatgpt) explore distributed task-based training through TreeServer, a system that exploits CPU parallelism to significantly advance the training efficiency of tree models.

On the front of explainability, Lundberg et al. (2020) contribute by developing a polynomial time algorithm for generating optimal local explanations, which also aids in understanding the global structure of the models [(Lundberg et al., 2020)](https://consensus.app/papers/from-explanations-trees-lundberg/14cd090e21985468a4b1f3bad8c1050b/?utm_source=chatgpt). They further refine these concepts in a subsequent study that proposes methods for comprehending the entire model structure through these local explanations [(Lundberg et al., 2019)](https://consensus.app/papers/trees-from-explanations-global-understanding-lundberg/0eed30cd9dc95907b3cf72fbd78a7d5f/?utm_source=chatgpt).

In the domain of application-specific advancements, [(Kern et al., 2019)](https://consensus.app/papers/treebased-machine-learning-methods-survey-research-kern/79f51bc04e9159d68514ee53ee7de112/?utm_source=chatgpt) review the application of tree-based methods in survey research, particularly in predicting nonresponse in panel surveys, showcasing the practical applicability of these techniques. Additionally, [(Pedretti et al., 2021)](https://consensus.app/papers/treebased-machine-learning-performed-analog-pedretti/178a7f7eec715eedab8b268e9dfc8b32/?utm_source=chatgpt) introduce a novel in-memory computation method using analog CAM to accelerate inference processes, thereby improving throughput for tree-based models.

Educational resources also play a crucial role in disseminating knowledge about these models. [(Kumar, 2019)](https://consensus.app/papers/treebased-modeling-techniques-kumar/edfe4216dec959799f6611485b63972e/?utm_source=chatgpt) offers a comprehensive guide on various tree-based modeling techniques, from basic decision trees to more complex structures like random forests and gradient boosting, aimed at enhancing understanding for new learners. [(Aldrich & Auret, 2013)](https://consensus.app/papers/treebased-methods-aldrich/daae7fa9145a596bbe6a905868ca4e32/?utm_source=chatgpt) provide an in-depth discussion on the construction, theory, and diverse applications of these methods, including their use in unsupervised fault diagnostic systems.

Lastly, addressing the challenge of evolving data streams, [(Ikonomovska et al., 2010)](https://consensus.app/papers/learning-model-trees-evolving-data-streams-ikonomovska/56d85d4d612753a1935a0b87eb8ad7b3/?utm_source=chatgpt) introduce an incremental stream mining algorithm for learning regression and model trees, equipped with mechanisms for drift detection and model adaptation. [(Frank et al., 1998)](https://consensus.app/papers/using-model-trees-classification-frank/0160e2c97b9c5a23bf09dc2dca8a82fd/?utm_source=chatgpt) explore the adaptability of model trees for classification problems, demonstrating how linear regression functions integrated into the trees can surpass traditional decision tree learners in accuracy.

This collection of works provides a robust foundation for understanding and advancing tree-based machine learning models,highlighting their versatility and adaptability across various scientific and practical domains.

#### 2.3. Model Stacking and Ensemble Techniques

In the field of ensemble learning, several notable studies have emerged that contribute significantly to both theoretical and practical aspects of model aggregation and performance enhancement. Rooney and Patterson [Rooney & Patterson, 2021](https://consensus.app/papers/combination-stacking-integration-rooney/dee73931fd1753eb86406057fbfcd2a5/?utm_source=chatgpt) introduced wMetaComb, an innovative approach that merges stacking with dynamic integration specifically for regression problems, demonstrating superiority over traditional single model and ensemble techniques. Similarly, [Silbert et al., 2021](https://consensus.app/papers/model-agnostic-combination-ensemble-learning-silbert/7ac9b7db1be455a291e17507202e7729/?utm_source=chatgpt) presented the Model Agnostic Combination (MAC), a robust ensembling technique that is invariant to the number of sub-models, showing strong performance across various challenges, including the Kaggle RSNA Intracranial Hemorrhage Detection.

[Zhou, 2021](https://consensus.app/papers/methods-horng/d7d9476b875b5b7aad737db5175e7b52/?utm_source=chatgpt) provided a comprehensive review of ensemble methods like bagging, boosting, and stacking, offering valuable insights into both their theoretical foundations and practical applications. [Džeroski & Ženko, 2021](https://consensus.app/papers/stacking-multiresponse-model-trees-džeroski/3ea9047b5a8a5d74968f74afebef427d/?utm_source=chatgpt) proposed a novel stacking method utilizing multi-response model trees, which showed improvements over traditional stacking methods and basic cross-validation techniques.

Further exploring the efficiency of ensemble techniques, [Cawood & van Zyl, 2021](https://consensus.app/papers/evaluating-state-forecasting-ensembles-metalearning-cawood/73196bfe67b655d18cdb9628290de42a/?utm_source=chatgpt) focused on the Exponential Smoothing-Recurrent Neural Network (ES-RNN), comparing its performance against various other ensembling strategies in forecasting contexts. [Pavlyshenko, 2021](https://consensus.app/papers/using-stacking-approaches-machine-learning-models-pavlyshenko/bcd49b370771530abd7924a59d8412a7/?utm_source=chatgpt) also highlighted the effectiveness of stacking techniques in improving ensemble performance for time series forecasting and logistic regression.

Lastly, the survey by [Mienye & Sun, 2021](https://consensus.app/papers/survey-ensemble-learning-concepts-algorithms-mienye/f3767787a7cc549bb850dcd5b5e07933/?utm_source=chatgpt) covered a broad range of ensemble methods, offering an in-depth look at their mathematical and algorithmic frameworks, which are crucial for both researchers and practitioners in the field. These studies collectively advance our understanding of how ensemble learning can be effectively utilized for model improvement across various domains.